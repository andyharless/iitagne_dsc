{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'problem'\n",
    "SOLUTION_DIR = '.'\n",
    "TRAIN_FN = 'training.csv'\n",
    "TEST_FN = 'testing.csv'\n",
    "SOLUTION_FN = 'solution.csv'\n",
    "SEP = '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = DATA_DIR + SEP + TRAIN_FN\n",
    "test_path = DATA_DIR + SEP + TEST_FN\n",
    "solution_path = SOLUTION_DIR + SEP + SOLUTION_FN\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "solution = pd.read_csv(solution_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add 'month' feature\n",
    "df_train['month'] = pd.to_datetime(df_train.Date).dt.month\n",
    "df_test['month'] = pd.to_datetime(df_test.Date).dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_range = df_train.Date<'2014-07-23'\n",
    "valid_range = df_train.Date>='2014-10-23'\n",
    "train = df_train[train_range]\n",
    "valid = df_train[valid_range]\n",
    "test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['Date', 'Identifier', 'Dep_Var'], axis=1)\n",
    "y_train = train.Dep_Var\n",
    "X_valid = valid.drop(['Date', 'Identifier', 'Dep_Var'], axis=1)\n",
    "y_valid = valid.Dep_Var\n",
    "X_test = test.drop(['Date', 'Identifier', 'Dep_Var'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Dep_Var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = QuantileTransformer(output_distribution='normal')\n",
    "y_train_gauss = pd.Series( q.fit_transform(y_train.values.reshape(-1, 1)).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check = q.inverse_transform(y_train_gauss.values.reshape(-1, 1)).reshape(-1)\n",
    "pd.DataFrame({'raw':y_train, 'check':check, 'gauss':y_train_gauss}).head()\n",
    "np.allclose(check, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit GBM Model to Training Subset and Predict Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train.astype(np.float32)\n",
    "d_train = lgb.Dataset(X, label=y_train_gauss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original validation result (without bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.10850310725319776, pvalue=7.577737140155415e-34)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {}\n",
    "params['learning_rate'] = 0.004 # 0.004 # shrinkage_rate\n",
    "params['sub_feature'] = 0.35    # feature_fraction (small values => use very different submodels)\n",
    "params['min_data'] = 500        # min_data_in_leaf\n",
    "params['max_depth'] = 2\n",
    "\n",
    "nboost=130\n",
    "\n",
    "reg = lgb.train(params, d_train, nboost)\n",
    "pred = reg.predict(X_valid)\n",
    "\n",
    "n_minus = 30\n",
    "params['max_depth'] = 3\n",
    "reg = lgb.train(params, d_train, nboost-n_minus)\n",
    "alpha = 0.5\n",
    "pred = alpha*pred + (1-alpha)*reg.predict(X_valid)\n",
    "\n",
    "n_plus = 40\n",
    "params['max_depth'] = 2\n",
    "params['bagging_freq'] = 5\n",
    "params['bagging_fraction'] = 0.78\n",
    "params['learning_rate'] = .005\n",
    "reg = lgb.train(params, d_train, nboost+n_plus)\n",
    "\n",
    "alpha = 0.78\n",
    "pred = alpha*pred + (1-alpha)*reg.predict(X_valid)\n",
    "\n",
    "pred_uniform = q.inverse_transform(pred.reshape(-1, 1)).reshape(-1)\n",
    "y_pred = np.round(pred_uniform).astype(np.int)\n",
    "\n",
    "spearmanr(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate validation result with bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.10470714057225206, pvalue=1.2661271970807093e-31)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['max_depth'] = 2\n",
    "nboost=130\n",
    "\n",
    "reg = lgb.train(params, d_train, nboost)\n",
    "pred = reg.predict(X_valid)\n",
    "\n",
    "n_minus = 30\n",
    "params['max_depth'] = 3\n",
    "reg = lgb.train(params, d_train, nboost-n_minus)\n",
    "alpha = 0.5\n",
    "pred = alpha*pred + (1-alpha)*reg.predict(X_valid)\n",
    "\n",
    "n_plus = 40\n",
    "params['max_depth'] = 2\n",
    "params['bagging_freq'] = 5\n",
    "params['bagging_fraction'] = 0.78\n",
    "params['learning_rate'] = .005\n",
    "reg = lgb.train(params, d_train, nboost+n_plus)\n",
    "\n",
    "alpha = 0.78\n",
    "pred = alpha*pred + (1-alpha)*reg.predict(X_valid)\n",
    "\n",
    "pred_uniform = q.inverse_transform(pred.reshape(-1, 1)).reshape(-1)\n",
    "y_pred = np.round(pred_uniform).astype(np.int)\n",
    "\n",
    "spearmanr(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit same model to full training data and predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigger = len(df_train)/len(train)\n",
    "full_nboost = int(nboost*bigger)\n",
    "n_minus = int(n_minus*bigger)\n",
    "n_plus = int(n_plus*bigger)\n",
    "full_nboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original test result (with bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.10933380428545146, pvalue=6.97545350538751e-35)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_X = df_train.drop(['Date', 'Identifier', 'Dep_Var'], axis=1)\n",
    "full_y = df_train.Dep_Var\n",
    "full_y_gauss = pd.Series(q.fit_transform(full_y.values.reshape(-1, 1)).reshape(-1))\n",
    "d_full_train = lgb.Dataset(full_X.values.astype(np.float32), label=full_y_gauss)\n",
    "\n",
    "params['max_depth'] = 2\n",
    "full_reg = lgb.train(params, d_full_train, full_nboost)\n",
    "test_pred_gauss = full_reg.predict(X_test)\n",
    "\n",
    "params['max_depth'] = 3\n",
    "full_reg = lgb.train(params, d_full_train, full_nboost-n_minus)\n",
    "alpha = 0.5\n",
    "test_pred_gauss = alpha*test_pred_gauss + (1-alpha)*full_reg.predict(X_test)\n",
    "\n",
    "params['max_depth'] = 2\n",
    "params['bagging_freq'] = 5\n",
    "params['bagging_fraction'] = 0.78\n",
    "params['learning_rate'] = .005\n",
    "full_reg = lgb.train(params, d_full_train, full_nboost+n_plus)\n",
    "alpha = 0.78\n",
    "test_pred_gauss = alpha*test_pred_gauss + (1-alpha)*full_reg.predict(X_test)\n",
    "\n",
    "\n",
    "test_pred_uniform = q.inverse_transform(test_pred_gauss.reshape(-1, 1)).reshape(-1)\n",
    "test_pred = np.round(test_pred_uniform).astype(np.int)\n",
    "spearmanr(solution.Dep_Var, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Apparently the test criterion used was slightly different, somehow taking into account consistency.  Hence the value here is different than the one reported for the competition, but it should be generally indicative of the relative result.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulated test result without bug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.10892773890449726, pvalue=1.2332473701811744e-34)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {}\n",
    "params['learning_rate'] = 0.004 # 0.004 # shrinkage_rate\n",
    "params['sub_feature'] = 0.35    # feature_fraction (small values => use very different submodels)\n",
    "params['min_data'] = 500        # min_data_in_leaf\n",
    "params['max_depth'] = 2\n",
    "full_reg = lgb.train(params, d_full_train, full_nboost)\n",
    "test_pred_gauss = full_reg.predict(X_test)\n",
    "\n",
    "params['max_depth'] = 3\n",
    "full_reg = lgb.train(params, d_full_train, full_nboost-n_minus)\n",
    "alpha = 0.5\n",
    "test_pred_gauss = alpha*test_pred_gauss + (1-alpha)*full_reg.predict(X_test)\n",
    "\n",
    "params['max_depth'] = 2\n",
    "params['bagging_freq'] = 5\n",
    "params['bagging_fraction'] = 0.78\n",
    "params['learning_rate'] = .005\n",
    "full_reg = lgb.train(params, d_full_train, full_nboost+n_plus)\n",
    "alpha = 0.78\n",
    "test_pred_gauss = alpha*test_pred_gauss + (1-alpha)*full_reg.predict(X_test)\n",
    "\n",
    "\n",
    "test_pred_uniform = q.inverse_transform(test_pred_gauss.reshape(-1, 1)).reshape(-1)\n",
    "test_pred = np.round(test_pred_uniform).astype(np.int)\n",
    "spearmanr(solution.Dep_Var, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the bug would have made the validation result worse but made the test result better.  \"Carelessness as a form of regularization\" is a thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
